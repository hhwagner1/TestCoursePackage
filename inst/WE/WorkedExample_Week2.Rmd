---
title: "Week 2: Spatial Data"
output:
  pdf_document: default
  html_document: default
  html_notebook: default
---
## 1. Overview of Worked Example

Author: Helene Wagner

This code builds on data and code from the 'GeNetIt' package by Jeff Evans and Melanie Murphy.

### a) Goals 

This worked example shows:

- How to import spatial coordinates and site attributes as spatially referenced data.  
- How to plot raster data in R and overlay sampling locations.
- How to calculate patch-level and land cover type - level landscape metrics.
- How to extract landscape data at sampling locations and within a buffer around them.

Try modifying the code to import your own data!

### b) Data set

This code uses landscape data and spatial coordinates from 30 locations where Colombia spotted frogs (Rana luteiventris) were sampled for the full data set analyzed by Funk et al. (2005) and Murphy et al. (2010). Please see the separate introduction to the data set.

- RALU_sites_all.csv: File with spatial coordinates and site attributes (preformatted for import, 30 rows x 19 columns).

We will extract values at sampling point locations and within a local neighborhood (buffer) from six raster layers, which are included with the 'GeNetIt' package (see Murphy et al. 2010 for definitions):

- cti:   compound topographic index
- err27: elevation relief ratio 
- ffp:   frost-free period
- gsp:   growing season precipitation
- hli:   heat load index
- nlcd:  national land cover data (categorical map)

### c) Required R libraries

```{r message=FALSE, warning=TRUE}
require(sp)
require(raster)
require(GeNetIt)
require(tmaptools) 
require(SDMTools) # for landscape metrics
```

### d) List of tasks

- Import site data from .CSV file into a 'SpatialPointsDataFrame' object (package 'sp').
- Display raster maps (package 'raster') and overlay sampling locations. Extract raster values at sampling locations.
- Calculate patch-level and class-level landscape metrics (package 'SDMTools').
- Extract landscape metrics at sampling locations.

## 2. Import site data from .csv file

### a) Import data into 'SpatialPointsDataFrame'

```{r}
RALU.site <- read.csv(system.file("extdata", "RALU_site_all.csv", 
                            package = "TestCoursePackage"), header=TRUE)
head(RALU.site)
```

The dataset has two columns with spatial coordinates and several attribute variables. 

So far, R treats the spatial coordinates like any other quantitative variables. To let R know this is spatial information, we import it into a spatial object type, a 'SpatialPointsDataFrame' from the 'sp' package.

The conversion is done with the function 'coordinates', which takes a data frame and converts it to a spatial object of the same name. The code is not very intuitive:

```{r}
RALU.site.sp <- RALU.site
coordinates(RALU.site.sp) <- ~coords.x1+coords.x2
head(RALU.site.sp)
```

Now R knows these are spatial data and knows how to handle them. It does not treat the coordinates as variables anymore, hence the first column is now 'SiteName'.

### b) Add spatial reference data

Before we can combine the sampling locations with other spatial datasets, such as raster data, we need to tell R where on earth these locations are (georeferencing). This is done by specifying the 'Coordinate Reference System' (CRS) or a 'proj4' string. 

For more information on CRS, see: https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/OverviewCoordinateReferenceSystems.pdf

We know that these coordinates are UTM zone 11 (Northern hemisphere) coordinates, hence we can use a helper function to find the correct 'proj4' string, using function 'get_proj4' from the 'tmaptools' package. (For the Southern hemisphere, you would add 's' after the zone: "utm11s").

```{r}
proj4string(RALU.site.sp) <- get_proj4("utm11")
```

If we had longitude and latitude coordinates, we would modify the command like this: 
proj4string(RALU.site.sp) <- get_proj4("longlat")

### c) Access data in 'SpatialPointsDataFrame'

As an S4 object, RALU.site.sp has predefined slots. These can be accessed with the @ symbol:

- @data: the attribute data
- @coords: the spatial coordinates
- @coords.nrs: the column numbers of the input data from which the coordinates were taken (filled automatically)
- @bbox: bounding box, i.e., the minimum and maximum of x and y coordinates (filled automatically)
- @proj4string: the georeferencing information

```{r}
slotNames(RALU.site.sp)
```

Here are the first few lines of the coordinates:

```{r}
head(RALU.site.sp@coords)
```

And the proj4 string:

```{r}
RALU.site.sp@proj4string
```

## 3. Display raster data and overlay sampling locations, extract data 

### a) Display raster data

The raster data for this project are already available in the package 'GeNetIt', under the name 'rasters', and we can load them with 'data(rasters)'. They are stored as a 'SpatialPixelsDataFrame', another S4 object type from the 'sp' package.

```{r}
data(rasters)
class(rasters)
```

However, raster data are better analyzed with the package 'raster', which has an object type 'raster'. - Maybe it was a bit confusing now to name our data 'rasters'. So let's rename it first to 'RALU.rasters.sp', then convert to a 'stack' of 'raster' object type (i.e. a set of raster layers with the same geometry).

```{r}
RALU.rasters.sp <- rasters
RALU.rasters.r <- stack(RALU.rasters.sp)
class(RALU.rasters.r)
```
Printing the name of the raster stack displays a summary. A few explanations:

- **dimensions**: number of rows (nrow), number of columns (ncol), number of cells (ncell), number of layers (nlayers). So we see there are 6 layers in the raster stack.
- **resolution**: cell size is 30 m both in x and y directions (typical for Landsat-derived remote sensing data)
- **coord.ref**: projected in UTM zone 11, though the 'datum' (NAD83) is different than what we used for the sampling locations. 

```{r}
RALU.rasters.r
```

Now we can use 'plot', which knows what to do with a raster stack.

Note: layer 'nlcd' is a categorical map of land cover types. See this week's bonus materials for how to better display a categorical map in R.

```{r}
plot(RALU.rasters.r)
```
Some layers seem to show a similar pattern. It is easy to calculate the correlation between quantitative raster layers. Here, the last layer 'ncld', is in fact categorical (land cover type), and it's correlation here is meaningless.

```{r}
layerStats(RALU.rasters.r, 'pearson', na.rm=T)
```

### b) Change color ramp, add sampling locations 

We can specify a color ramp by setting the 'col' argument. The default is 'terrain.colors(255)'. Here we change it to 'rainbow(10)', a rainbow colorpalette with 10 color levels.

Note: To learn about options for the 'plot' function for 'raster' objects, access the help file by typing '?plot' and select 'Plot a Raster* object'.

And we can add the sampling locations (if we plot only a single raster layer). Here we use 'rev' to reverse the color ramp for plotting raster layer 'ffp', and add the sites as white circles with black outlines.

```{r}
plot(raster(RALU.rasters.r, layer="ffp"), col=rev(rainbow(9)))
points(RALU.site.sp, pch=21, col="black", bg="white")
```

### Extract raster values at sampling locations

The following code adds six variables to the data slot of RALU.site.sp. Technically we combine the columns of the existing data frame 'RALU.site.sp' with the new columns in a new dat frame with the same name. 

R notices the difference in projection (CRS) between the sampling point data and the rasters and takes care of it, providing just a warning. 

```{r}
RALU.site.sp@data <- data.frame(RALU.site.sp@data, extract(RALU.rasters.r, RALU.site.sp))
```
What land cover type is assigned to the most sampling units? Let's tabulate them.

Note: land cover types are coded by numbers. The most frequent type is '42'. Check here what the numbers mean: https://www.mrlc.gov/nlcd06_leg.php

```{r}
table(RALU.site.sp@data$nlcd)
```

## 4. Calculate patch-level and class-level landscape metrics

### a) Calculate class-level landscape metrics

Here we evaluate the spatial distribution of each cover type (class - this is not the same here as an object class). This is extremely fast in R. But first we'll extract the 'nlcd' raster layer in a separate raster 'NLCD' to simplify the code.

```{r}
NLCD <- raster(RALU.rasters.r, layer="nlcd")
NLCD.class <- ClassStat(NLCD,cellsize=30)
```

For a list of all 37 metrics calculated, check the helpfile for 'ClassStat'. Background  information is available on the Fragstats webpage: http://www.umass.edu/landeco/research/fragstats/documents/Metrics/Metrics%20TOC.htm

```{r}
?ClassStat
```

### b) Calculate patch-level landscape metrics for 'Evergreen Forest'

Calculating patch-level metrics is a little more involved, as we have to decide which cover type (class) to analyze, and then delinate patches for that cover type. Then we calculate statistics for each patch.

The first step is to reduce the land cover map 'nlcd' to a binary map showing forest vs. non-forest ('Everygreen Forest' is the only forest type mapped in the study area). We can do this by using a logical test: 'RALU.rasters.r==42', which tests for each cell in NLCD whether it is equal to 42. This results in a binary map, which we can plot, and overlay the sampling locations.

```{r}
plot(NLCD==42)
points(RALU.site.sp, pch=21, bg="yellow", col="black")
```

We use the function 'ConnCompLabel' to delineate patches (with the 8-neighbor rule, other rules are not implemented). This creates a new raster where the value in each cell is the new patch ID if forest (NLCD==42), or zero if not. Then we run 'PatchStat' on the new raster.

```{r}
ccl.mat <- ConnCompLabel(NLCD==42)
NLCD.patch <- PatchStat(ccl.mat,cellsize=30)
dim(NLCD.patch)
```

This returns a list of 223 forest patches (rows) and 12 patch-level landscape metrics (columns). Let's look at the first few patches. Patches differ greatly in size! 

```{r}
head(NLCD.patch)
```

For a list of the patch-level metrics calculated, check the helpfile.

```{r}
?PatchStat
```

Let's add forest patch size to the RALU.site.sp data. First we need to get the patch ID at each sampling location, then its size.

```{r}
a <- extract.data(RALU.site.sp@coords, ccl.mat) # get patch IDs
a[a==0] <- NA
RALU.site.sp@data$ForestPatchSize <- NLCD.patch[a,"area"]
RALU.site.sp@data$ForestPatchSize[is.na(a)] <- 0
RALU.site.sp@data$ForestPatchSize
```
 Plot a bubble map of forest patch size at each sampling location:
 
```{r}
bubble(RALU.site.sp, "ForestPatchSize", fill=FALSE, key.entries=as.numeric(names(table(RALU.site.sp@data$ForestPatchSize))))
```


## Extract landscape metrics at sampling locations.

### a) Calculate class-level metrics in buffer around sampling locations

First we define the buffer radius (in meters) and cell size:

```{r}
Radius <- 500    # Define buffer radius
Cellsize <- 30   # Indicate cell size in meters
```

Then we create a loop through all sampling locations, calculating class-level metrics for each one within its buffer. I'm afraid that explaining this code in detail would be too much for today.

```{r}
RALU.site.class <- list()

for(i in 1:length(RALU.site.sp))
{
  dist <- distanceFromPoints(NLCD, RALU.site.sp@coords[i,])
  test <- dist
  test[test < Cellsize] <- 1
  test[test > 1] <- NA
  test.buffer <- buffer(test, Radius)
  test2 <- NLCD * test.buffer
  #plot(test2) # Just checking: plot buffer
  RALU.site.class[[i]] <- ClassStat(test2,cellsize=30)
}
names(RALU.site.class) <- RALU.site.sp@data$SiteName

# Make sure all sites list all cover types, even if they are absent from buffer:
class.ID <- levels(as.factor(NLCD))[[1]]
RALU.site.class <- lapply(RALU.site.class, function(ls) merge(class.ID, ls, all=TRUE, by.x="ID", by.y="class"))
```

### b) Extract landscape metric of choice for a single cover type (as vector)

Now we can extract any variable of interest for any cover type of interest. Here we'll extract the percentage of (evergreen) forest within a 500 m radius around each site.

```{r}
# Extract variable 'prop.landscape' for cover type 42 (Evergreen Forest): 
PercentForest500 <- unlist(lapply(RALU.site.class, function(ls) ls[ls$ID==42, "prop.landscape"]))
PercentForest500[is.na(PercentForest500)] <- 0
PercentForest500
```
### c) Extract landscape metric of choice for all cover types (as data frame)

To extract the landscape metric 'prop.landscape' for all cover types as a data.frame (one column per cover type), use this code. 

We'll define column names combining 'Prop' for 'proportion of landscape', '500' to indicate the 500 m buffer radius, and the ID of each cover type.

```{r}
tmp <- Reduce(rbind,lapply(RALU.site.class, function(ls) ls[, "prop.landscape"]))
dimnames(tmp) <- list(row.names=names(RALU.site.class), 
                      col.names=paste("Prop.500", class.ID$ID, sep="."))
tmp[is.na(tmp)] <- 0
RALU.prop.landscape500 <- as.data.frame(tmp)
head(RALU.prop.landscape500)
```

### c) Append to site data set

```{r}
RALU.site.sp@data <- data.frame(RALU.site.sp@data, RALU.prop.landscape500) 
```
Note: check this week's bonus material if you want to see how to use the new 'sf' library for spatial data, and how to export the site data to an shapefile that you can import into a GIS.
